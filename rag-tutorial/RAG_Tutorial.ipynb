{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a32c596",
   "metadata": {},
   "source": [
    "**What is Retrieval Augmented Generation (RAG)?**\n",
    "\n",
    "AI are trained on large sets of information, but that information can quickly become outdated or may not cover the specific details you care about. For example, if you ask an AI about a recent event or something unique to your company, it might not know the answer or could even make up a response that sounds correct but isn’t accurate. This is known as a “hallucination,” and it’s a common challenge with AIs.\n",
    "\n",
    "RAG helps solve this problem by adding an additoinal step that uses an AI to search for relavant data in real time and use that information to answer questions. Instead of guessing, the AI can pull up the exact details from your files, policies, or records, making its answers more reliable and easier to verify.\n",
    "\n",
    "To enable RAG, an important step is preparing your data so it is searchable. This usually means storing your documents in a format that allows the AI to quickly retrieve specific passages. In practice, this could involve setting up a specialized database for documents (often a “vector database”) or indexing your existing content in a way that supports efficient lookups. In some cases, it may also involve cleaning or restructuring your data to ensure consistency.\n",
    "\n",
    "In the following notebook you will take a deep look at the steps required to setup your own RAG system. In this example PDF files will be used to help give more information to the LLM about the attention layer using the \"Attention Is All You Need\" PDF.\n",
    "\n",
    "![RAG Flow](rag-architecture-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e7008",
   "metadata": {},
   "source": [
    "---\n",
    "First, install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4a3e7",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.26\n",
    "!pip install langchain_chroma==0.2.5\n",
    "!pip install langchain_community==0.3.27\n",
    "!pip install langchain-together==0.3.1\n",
    "!pip install pypdf==5.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f447e",
   "metadata": {},
   "source": [
    "---\n",
    "Environment variables will be configured for use throughout the notebook. This approach centralizes important information, making it easier to update settings in the future. For example, you can quickly change the embedding model or update the API key if it expires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b668c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = \"YOUR_KEY\"\n",
    "DOC_PATH = \"docs\"\n",
    "CHROMA_PATH = \"chroma_vectors\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1f8a9",
   "metadata": {},
   "source": [
    "---\n",
    "Download a PDF for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d513f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p docs\n",
    "!wget -O docs/attention-genai.pdf https://raw.githubusercontent.com/Brian-McGinn/GenAI-Tutorials/main/rag-tutorial/docs/attention-genai.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d983e",
   "metadata": {},
   "source": [
    "---\n",
    "Before running any models, it is important to load the documents. The following function is designed to extract documents from a directory and create a list of Document objects using the LangChain schema. This example includes support for PDF and Markdown files, but it can be extended to accommodate additional file types as needed. For more information, refer to the [LangChain Document Loaders documentation](https://python.langchain.com/docs/integrations/document_loaders/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e3933",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "def load_documents_from_directory(directory: str=DOC_PATH) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load all PDF and Markdown files from a directory into LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            documents.extend(loader.load())\n",
    "        \n",
    "        elif filename.lower().endswith(\".md\"):\n",
    "            loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "            documents.extend(loader.load())\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# Execute Documentation Load\n",
    "docs = load_documents_from_directory(DOC_PATH)\n",
    "print(docs[0].page_content.splitlines()[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046560a8",
   "metadata": {},
   "source": [
    "---\n",
    "With the documents now loaded, they can be split into smaller chunks to provide a more refined context window. While it is possible to manually split the document contents, LangChain and other APIs offer methods to streamline this process. In this example, the RecursiveCharacterTextSplitter method will be used to divide the document content into 500-character chunks. To help avoid breaking up sentences or words in the middle, a chunk overlap can be specified. This means that the next chunk will begin a set number of characters before the end of the previous chunk, thereby capturing any content that may have been cut off.\n",
    "\n",
    "For more information, see [Recursive Character Text Splitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2375e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    split_text = text_splitter.split_documents(documents)\n",
    "    return split_text\n",
    "\n",
    "# split documents loaded in the previous step\n",
    "split_docs = split_documents(docs)\n",
    "print(split_docs[0].page_content[:156])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e817a5a-ad2c-4a27-b725-3ebef8e0a123",
   "metadata": {},
   "source": [
    "---\n",
    "As demonstrated in the previous example, the page content is divided into separate chunks.\n",
    "\n",
    "Additionally, the RecursiveCharacterTextSplitter() function automatically adds metadata to each data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842f574-b81b-4178-9394-ad24aebc05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97dfbcc",
   "metadata": {},
   "source": [
    "---\n",
    "You can add custom metadata to be used later for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce48c45-f3ed-4dc3-b258-11fbe8d3a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs[0].metadata[\"keywords\"] = \"RAG-Tutorial\"\n",
    "split_docs[0].metadata[\"NewKey\"] = \"Example key\"\n",
    "\n",
    "print(split_docs[0].metadata[\"keywords\"])\n",
    "print(split_docs[0].metadata[\"NewKey\"])\n",
    "print(split_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be4a8e",
   "metadata": {},
   "source": [
    "---\n",
    "Next, generate embeddings for the chunks using TogetherEmbeddings and the BAAI/bge-base-en-v1.5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import TogetherEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def save_vectors(documents: list[Document]):\n",
    "    embedding = TogetherEmbeddings(model=EMBEDDING_MODEL, api_key=TOGETHER_API_KEY)\n",
    "    Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "    )\n",
    "\n",
    "# Save the previously created chunks into a local Chroma vector database\n",
    "save_vectors(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49004cfd",
   "metadata": {},
   "source": [
    "---\n",
    "One method for retrieving data is similarity search. This technique uses the user query and embeddings to identify the most relevant document chunks. By specifying the parameter k, you can return the top k most similar chunks found during the search. Using a smaller k value reduces resource usage and latency but may provide less context for the prompt. Adjusting the k value is important for balancing performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(query):\n",
    "    if query:\n",
    "        embedding = TogetherEmbeddings(model=EMBEDDING_MODEL, api_key=TOGETHER_API_KEY)\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        return vector_store.similarity_search(query, k=3)\n",
    "    return \"No Results Found\"\n",
    "\n",
    "# Get the context chunks based on query \n",
    "sim_query = \"What is a transformation attention layer?\"\n",
    "sim_context = get_embeddings(sim_query)\n",
    "print(sim_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30366bc8",
   "metadata": {},
   "source": [
    "---\n",
    "Metadata can also be used to refine your search. This is especially beneficial when searching large datasets or when you know which metadata will yield the most relevant response to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a64e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_attention(query):\n",
    "    if query:\n",
    "        embedding = TogetherEmbeddings(model=EMBEDDING_MODEL, api_key=TOGETHER_API_KEY)\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        return vector_store.similarity_search(query, k=3, filter={'source': 'docs/attention-genai.pdf'})\n",
    "    return \"No Results Found\"\n",
    " \n",
    "# Get the context chunks based on query and only from the attention-genai.pdf metadata\n",
    "meta_query = \"Explain the Transformer model architecture.\"\n",
    "meta_context = get_embeddings_from_attention(meta_query)\n",
    "print(meta_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ba5db",
   "metadata": {},
   "source": [
    "---\n",
    "With the RAG functions complete and executed, it is time to create the prompt templates. Notice that the system prompt explicitly instructs the AI to use only the provided context when answering questions. This ensures that the AI focuses its responses on the supplied data chunks rather than general knowledge. By instructing the AI not to answer when it does not know, we reduce the possibility of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_message = (\n",
    "    \"You are a helpful assistant that ONLY answers questions based on the \"\n",
    "    \"provided context. If no relevant context is provided, do NOT answer the query.\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    )\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241e499",
   "metadata": {},
   "source": [
    "---\n",
    "With the prompts configured, it is time to set up an LLM and query the AI using the RAG chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26270dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    api_key=TOGETHER_API_KEY,\n",
    "    max_tokens=1000,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def format_context(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# User the LLM and retrieve context in the previous \n",
    "messages = prompt.format_messages(\n",
    "    context=format_context(meta_context),\n",
    "    query=meta_query\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3e748",
   "metadata": {},
   "source": [
    "---\n",
    "**Handling Large-Scale Data:**\n",
    "\n",
    "While the basic RAG approach covered in this tutorial will work for most datasets, it is important to be aware of advanced techniques that can further enhance accuracy.\n",
    "\n",
    "- ReRanking: By passing your initial similarity search results to a reranking model (e.g., Salesforce/Llama-Rank-V1), the AI can recalculate the relevance scores of the results and provide a more refined set of context passages. This method can also be used to narrow down search results by retrieving a large number of results in the initial search and then selecting a smaller, more relevant subset with the reranking model. The primary drawback is that this approach requires two searches, which may impact user performance.\n",
    "\n",
    "- GraphRAG: This approach involves creating a knowledge graph of your data that connects each data point to other relevant data points. Leveraging these relationships helps the search return connected information and provides explanations for those connections. Although this method can yield higher accuracy, transforming your data into a knowledge graph can be computationally expensive.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
